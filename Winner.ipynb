{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "118c853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, log_loss, roc_auc_score, matthews_corrcoef, cohen_kappa_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "978b0dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('Data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "56fb924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = data.drop(columns=['Rated_(T/F)', 'Winner', 'Game_Status'])\n",
    "y = data['Winner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "47d17b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "847ba56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for dimensionality reduction\n",
    "pca = PCA(n_components=15)  # Reducing to 15 components\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8d8e093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: 95.49578201410569\n"
     ]
    }
   ],
   "source": [
    "# Print explained variance\n",
    "print(\"Explained Variance Ratio:\", sum(pca.explained_variance_ratio_)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "90409c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2737bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE (Synthetic Minority Over-sampling Technique) to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ee94e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. **RandomForestClassifier with Hyperparameter Tuning**\n",
    "# Hyperparameter search space for RandomForest\n",
    "search_space_rf = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [5, 10, 20, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f14569c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stratified K-Fold Cross-Validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b2e7a542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kshitish Pandit\\Desktop\\Chess\\env\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    }
   ],
   "source": [
    "# RandomForest model with BayesSearchCV for hyperparameter tuning\n",
    "grid_search_rf = BayesSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    search_spaces=search_space_rf,\n",
    "    n_iter=20,  # Number of iterations for search\n",
    "    cv=stratified_kfold,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_rf.fit(X_resampled, y_resampled)\n",
    "y_pred_rf = grid_search_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d79a7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. **LightGBM Classifier**\n",
    "# Label encoding\n",
    "label_mapping = {'black': 1, 'draw': 0, 'white': 2}\n",
    "y_train_encoded = np.array([label_mapping.get(label, -1) for label in y_train])\n",
    "y_test_encoded = np.array([label_mapping.get(label, -1) for label in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2666c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no undefined labels\n",
    "assert np.all(y_train_encoded != -1), \"Some labels in y_train are undefined\"\n",
    "assert np.all(y_test_encoded != -1), \"Some labels in y_test are undefined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ab69abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the training data using SMOTE\n",
    "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "de52b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM model parameters\n",
    "params_lgb = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'metric': 'multi_logloss',\n",
    "    'scale_pos_weight': 5,  # Adjusting for class imbalance\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_bin': 255\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fd57a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LightGBM dataset and train\n",
    "train_data = lgb.Dataset(X_train_oversampled, label=y_train_oversampled)\n",
    "test_data = lgb.Dataset(X_test, label=y_test_encoded, reference=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1e4d02f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004511 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3825\n",
      "[LightGBM] [Info] Number of data points in the train set: 10524, number of used features: 15\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "model_lgb = lgb.train(params_lgb, train_data, num_boost_round=150, valid_sets=[test_data])\n",
    "preds_lgb = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n",
    "pred_labels_lgb = np.argmax(preds_lgb, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "45879bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. **Support Vector Machine (SVM)**\n",
    "svm_model = SVC(kernel='rbf', class_weight='balanced', probability=True)  # Handling imbalance\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1ace3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. **XGBoost Classifier**\n",
    "# Encode labels for XGBoost\n",
    "y_train_encoded_xgb = np.array([label_mapping.get(label, -1) for label in y_train])\n",
    "y_test_encoded_xgb = np.array([label_mapping.get(label, -1) for label in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f1662c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no undefined labels\n",
    "assert np.all(y_train_encoded_xgb != -1), \"Some labels in y_train are undefined\"\n",
    "assert np.all(y_test_encoded_xgb != -1), \"Some labels in y_test are undefined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ba53baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scale_pos_weight for XGBoost (for class imbalance)\n",
    "scale_pos_weight = (np.sum(y_train_encoded_xgb == 1) + np.sum(y_train_encoded_xgb == 2)) / np.sum(y_train_encoded_xgb == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2543ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameters\n",
    "params_xgb = {\n",
    "    'objective': 'multi:softmax',  # Multi-class classification\n",
    "    'num_class': 3,                # 3 classes: draw, black, white\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.1,\n",
    "    'scale_pos_weight': scale_pos_weight\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bddcf2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kshitish Pandit\\Desktop\\Chess\\env\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [16:20:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost model\n",
    "model_xgb = xgb.train(params=params_xgb, \n",
    "                      dtrain=xgb.DMatrix(X_train, label=y_train_encoded_xgb), \n",
    "                      num_boost_round=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "54cc0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with XGBoost\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "# Convert X_train to a DataFrame (if not already done)\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "X_test_df = X_test_df[X_train_df.columns]\n",
    "xgb_test = xgb.DMatrix(X_test_df)\n",
    "preds_xgb = model_xgb.predict(xgb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d4b90129",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "models = {\n",
    "    \"Random Forest\": grid_search_rf,\n",
    "    \"LightGBM\": model_lgb,\n",
    "    \"SVM\": svm_model,\n",
    "    \"XGBoost\": model_xgb\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4a30f125",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_prob shape: (1745, 3)\n",
      "Sum of probabilities for each sample: [1. 1. 1. ... 1. 1. 1.]\n",
      "\n",
      "Random Forest Accuracy: 37.36%\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.03      0.28      0.05        71\n",
      "           1       0.48      0.09      0.15       803\n",
      "           2       0.64      0.65      0.64       871\n",
      "\n",
      "    accuracy                           0.37      1745\n",
      "   macro avg       0.38      0.34      0.28      1745\n",
      "weighted avg       0.54      0.37      0.39      1745\n",
      "\n",
      "\n",
      "Random Forest Confusion Matrix:\n",
      "[[ 20  22  29]\n",
      " [448  70 285]\n",
      " [254  55 562]]\n",
      "\n",
      "Random Forest Log Loss: 1.3904\n",
      "\n",
      "Random Forest ROC-AUC: 0.5304\n",
      "\n",
      "Random Forest Matthews Correlation Coefficient (MCC): 0.1219\n",
      "\n",
      "Random Forest Cohen's Kappa Score: 0.0972\n",
      "y_pred_prob shape: (1745, 3)\n",
      "Sum of probabilities for each sample: [1. 1. 1. ... 1. 1. 1.]\n",
      "\n",
      "LightGBM Accuracy: 57.02%\n",
      "\n",
      "LightGBM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.24      0.13        71\n",
      "           1       0.62      0.54      0.58       803\n",
      "           2       0.64      0.62      0.63       871\n",
      "\n",
      "    accuracy                           0.57      1745\n",
      "   macro avg       0.45      0.47      0.45      1745\n",
      "weighted avg       0.61      0.57      0.59      1745\n",
      "\n",
      "\n",
      "LightGBM Confusion Matrix:\n",
      "[[ 17  21  33]\n",
      " [ 92 437 274]\n",
      " [ 78 252 541]]\n",
      "\n",
      "LightGBM Log Loss: 0.8688\n",
      "\n",
      "LightGBM ROC-AUC: 0.6678\n",
      "\n",
      "LightGBM Matthews Correlation Coefficient (MCC): 0.2423\n",
      "\n",
      "LightGBM Cohen's Kappa Score: 0.2404\n",
      "y_pred_prob shape: (1745, 3)\n",
      "Sum of probabilities for each sample: [1. 1. 1. ... 1. 1. 1.]\n",
      "\n",
      "SVM Accuracy: 37.13%\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.44      0.07        71\n",
      "           1       0.00      0.00      0.00       803\n",
      "           2       0.63      0.71      0.66       871\n",
      "\n",
      "    accuracy                           0.37      1745\n",
      "   macro avg       0.22      0.38      0.25      1745\n",
      "weighted avg       0.31      0.37      0.33      1745\n",
      "\n",
      "\n",
      "SVM Confusion Matrix:\n",
      "[[ 31   0  40]\n",
      " [475   0 328]\n",
      " [254   0 617]]\n",
      "\n",
      "SVM Log Loss: 1.8240\n",
      "\n",
      "SVM ROC-AUC: 0.5616\n",
      "\n",
      "SVM Matthews Correlation Coefficient (MCC): 0.1398\n",
      "\n",
      "SVM Cohen's Kappa Score: 0.1026\n",
      "y_pred_prob shape: (1745,)\n",
      "y_pred_prob is 1D. Using labels directly.\n",
      "y_pred_prob shape: (1745,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kshitish Pandit\\Desktop\\Chess\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Kshitish Pandit\\Desktop\\Chess\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Kshitish Pandit\\Desktop\\Chess\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[190], line 71\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Cohen\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Kappa Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkappa_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 71\u001b[0m     \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[190], line 40\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, model_name, X_test, y_test_encoded, X_test_df)\u001b[0m\n\u001b[0;32m     37\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(y_pred)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred_prob shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_pred_prob\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of probabilities for each sample:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# This should be close to 1\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Accuracy\u001b[39;00m\n\u001b[0;32m     43\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_encoded, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\Kshitish Pandit\\Desktop\\Chess\\env\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:2389\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2386\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2390\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\n\u001b[0;32m   2392\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kshitish Pandit\\Desktop\\Chess\\env\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, model_name, X_test, y_test_encoded, X_test_df=None):\n",
    "    # Predictions for different models\n",
    "    if model_name == \"LightGBM\":\n",
    "        # Get predictions and pick the class with the highest probability\n",
    "        y_pred = np.argmax(model.predict(X_test, num_iteration=model.best_iteration), axis=1)\n",
    "        y_pred_prob = model.predict(X_test, num_iteration=model.best_iteration)  # Probabilities for LightGBM\n",
    "        \n",
    "    elif model_name == \"XGBoost\":\n",
    "        # XGBoost: Use model.predict() for probabilities (multi-class classification)\n",
    "        xgb_test = xgb.DMatrix(X_test_df)\n",
    "        y_pred_prob = model.predict(xgb_test, output_margin=False)  # Probabilities for each class\n",
    "        \n",
    "        # Check the shape of y_pred_prob to confirm it's a 2D array\n",
    "        print(f\"y_pred_prob shape: {y_pred_prob.shape}\")\n",
    "        \n",
    "        # If y_pred_prob is 1D (only class labels), adjust accordingly\n",
    "        if len(y_pred_prob.shape) == 1:  # 1D array (predicted labels)\n",
    "            y_pred = y_pred_prob  # Use labels directly\n",
    "            print(\"y_pred_prob is 1D. Using labels directly.\")\n",
    "        else:  # 2D array (probabilities for each class)\n",
    "            print(f\"Sum of probabilities for each sample: {np.sum(y_pred_prob, axis=1)}\")  # Should sum to 1\n",
    "            y_pred = np.argmax(y_pred_prob, axis=1)  # Convert probabilities to class labels\n",
    "            print(\"y_pred_prob is 2D. Using np.argmax for class labels.\")\n",
    "                 \n",
    "    elif model_name == \"SVM\":\n",
    "        # SVM: Get probabilities for each class\n",
    "        y_pred_prob = model.predict_proba(X_test)  # Get class probabilities\n",
    "        y_pred = np.argmax(y_pred_prob, axis=1)  # Predicted class labels (choose class with highest probability)\n",
    "        \n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_prob = model.predict_proba(X_test)  # Probabilities for other models\n",
    "    \n",
    "    # If predictions are in string format, convert them to numeric labels using the label_encoder\n",
    "    if isinstance(y_pred[0], str):\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_pred = label_encoder.fit_transform(y_pred)\n",
    "    \n",
    "    print(\"y_pred_prob shape:\", y_pred_prob.shape)\n",
    "    print(\"Sum of probabilities for each sample:\", np.sum(y_pred_prob, axis=1))  # This should be close to 1\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "    print(f\"\\n{model_name} Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Classification Report\n",
    "    print(f\"\\n{model_name} Classification Report:\\n{classification_report(y_test_encoded, y_pred)}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test_encoded, y_pred)\n",
    "    print(f\"\\n{model_name} Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    # Log Loss (for all models, using predicted probabilities)\n",
    "    log_loss_value = log_loss(y_test_encoded, y_pred_prob)\n",
    "    print(f\"\\n{model_name} Log Loss: {log_loss_value:.4f}\")\n",
    "\n",
    "    # ROC-AUC (for all models, using predicted probabilities)\n",
    "    roc_auc = roc_auc_score(y_test_encoded, y_pred_prob, multi_class='ovr')\n",
    "    print(f\"\\n{model_name} ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Matthews Correlation Coefficient (MCC)\n",
    "    mcc_value = matthews_corrcoef(y_test_encoded, y_pred)\n",
    "    print(f\"\\n{model_name} Matthews Correlation Coefficient (MCC): {mcc_value:.4f}\")\n",
    "\n",
    "    # Cohen's Kappa Score\n",
    "    kappa_value = cohen_kappa_score(y_test_encoded, y_pred)\n",
    "    print(f\"\\n{model_name} Cohen's Kappa Score: {kappa_value:.4f}\")\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    evaluate_model(model, model_name, X_test, y_test_encoded, X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcabd35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
